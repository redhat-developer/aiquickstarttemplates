apiVersion: scaffolder.backstage.io/v1beta3
kind: Template
metadata:
  name: redhat-ai-enterprise-rag-chatbot
  title: Enterprise RAG Chatbot
  description: Deploy a Retrieval-Augmented Generation chatbot based on rh-ai-quickstart/RAG with company-specific data integration
  tags:
    - redhat
    - ai
    - openshift-ai
    - rag
    - chatbot
    - pgvector
    - llm
    - llama-stack
spec:
  owner: platform-team
  type: service

  parameters:
    - title: Project Information
      required:
        - name
        - owner
      properties:
        name:
          title: Project Name
          type: string
          description: Name for your RAG chatbot instance
          ui:autofocus: true
          ui:field: EntityNamePicker
        owner:
          title: Owner
          type: string
          description: Team or individual owning this chatbot
          ui:field: OwnerPicker
          ui:options:
            catalogFilter:
              kind: [Group, User]
        description:
          title: Description
          type: string
          description: Purpose of this RAG chatbot
          default: Enterprise RAG chatbot with company-specific knowledge

    - title: Deployment Configuration
      required:
        - deploymentMode
        - namespace
      properties:
        deploymentMode:
          title: Deployment Mode
          type: string
          description: Choose deployment target
          default: openshift
          enum:
            - local
            - openshift
          enumNames:
            - Local (Podman/Docker with Ollama)
            - OpenShift (Helm chart with vLLM)
        openshiftCluster:
          title: OpenShift Cluster
          type: string
          description: Target OpenShift cluster URL
          default: https://api.cluster.example.com:6443
          ui:hidden:
            deploymentMode: local
        namespace:
          title: Namespace/Project
          type: string
          description: Namespace for deployment
          default: rag-chatbot

    - title: LLM Configuration
      required:
        - llmModel
      properties:
        llmModel:
          title: LLM Model
          type: string
          description: Model for text generation
          default: meta-llama/Llama-3.2-3B-Instruct
          enum:
            - meta-llama/Llama-3.2-3B-Instruct
            - meta-llama/Llama-3.1-8B-Instruct
            - meta-llama/Llama-3.1-70B-Instruct
          enumNames:
            - Llama 3.2 3B (Recommended for local/CPU)
            - Llama 3.1 8B (Requires GPU)
            - Llama 3.1 70B (Requires multiple GPUs)
        embeddingModel:
          title: Embedding Model
          type: string
          description: Model for generating text embeddings
          default: all-MiniLM-L6-v2
          enum:
            - all-MiniLM-L6-v2
            - BAAI/bge-small-en-v1.5
            - BAAI/bge-large-en-v1.5
        enableLlamaGuard:
          title: Enable Llama Guard
          type: boolean
          description: Enable safety filtering with Llama Guard model
          default: false
          ui:help: Recommended for production deployments

    - title: Hardware Configuration
      properties:
        accelerator:
          title: Hardware Accelerator
          type: string
          description: Hardware acceleration for model inference
          default: cpu
          enum:
            - cpu
            - nvidia-gpu
            - intel-gaudi
          enumNames:
            - CPU (Xeon) - No GPU required
            - NVIDIA GPU (L4, A100)
            - Intel Gaudi HPU
          ui:hidden:
            deploymentMode: local
        vramSize:
          title: VRAM Size
          type: string
          description: Minimum VRAM requirement
          default: 24GB
          enum:
            - 24GB
            - 48GB
            - 80GB
          ui:hidden:
            accelerator: cpu

    - title: Vector Database Configuration
      properties:
        pgvectorStorageSize:
          title: PGVector Storage Size
          type: string
          description: Persistent volume size for PostgreSQL/PGVector
          default: 50Gi
        pgvectorMemory:
          title: PGVector Memory
          type: string
          description: Memory allocation for PostgreSQL
          default: 4Gi

    - title: Document Ingestion Configuration
      properties:
        dataSources:
          title: Data Source Types
          type: array
          description: Select data sources to ingest
          items:
            type: string
            enum:
              - github
              - s3-minio
              - url
              - confluence
              - sharepoint
          uniqueItems: true
          default:
            - url
        githubRepos:
          title: GitHub Repositories
          type: string
          description: Comma-separated list of GitHub repos (org/repo format)
          ui:help: Example - redhat-developer/redhat-helm-charts,openshift/documentation
          ui:hidden:
            dataSources:
              not:
                contains:
                  const: github
        githubToken:
          title: GitHub Token
          type: string
          description: GitHub PAT for private repository access
          ui:widget: password
          ui:hidden:
            dataSources:
              not:
                contains:
                  const: github
        s3Endpoint:
          title: S3/MinIO Endpoint
          type: string
          description: S3-compatible storage endpoint
          default: https://s3.amazonaws.com
          ui:hidden:
            dataSources:
              not:
                contains:
                  const: s3-minio
        s3Bucket:
          title: S3 Bucket
          type: string
          description: Bucket name containing documents
          ui:hidden:
            dataSources:
              not:
                contains:
                  const: s3-minio
        s3AccessKey:
          title: S3 Access Key
          type: string
          ui:hidden:
            dataSources:
              not:
                contains:
                  const: s3-minio
        s3SecretKey:
          title: S3 Secret Key
          type: string
          ui:widget: password
          ui:hidden:
            dataSources:
              not:
                contains:
                  const: s3-minio
        documentUrls:
          title: Document URLs
          type: string
          description: Comma-separated list of PDF URLs to ingest
          ui:help: Example - https://example.com/doc1.pdf,https://example.com/doc2.pdf
          ui:hidden:
            dataSources:
              not:
                contains:
                  const: url
        enableAutoIngestion:
          title: Enable Auto Ingestion
          type: boolean
          description: Automatically re-ingest documents on schedule
          default: false
        ingestionSchedule:
          title: Ingestion Schedule
          type: string
          description: Cron schedule for document re-ingestion
          default: "0 2 * * *"
          ui:help: Daily at 2 AM (UTC)
          ui:hidden:
            enableAutoIngestion: false

    - title: UI Configuration
      properties:
        enableStreamlit:
          title: Enable Streamlit UI
          type: boolean
          description: Deploy Streamlit-based chat interface
          default: true
        uiPort:
          title: UI Port
          type: number
          description: Port for Streamlit UI
          default: 8501
          ui:hidden:
            enableStreamlit: false

    - title: Advanced Configuration
      properties:
        enableKubeflowPipelines:
          title: Enable Kubeflow Pipelines
          type: boolean
          description: Use Kubeflow for ingestion orchestration
          default: false
          ui:help: Recommended for OpenShift AI deployments
          ui:hidden:
            deploymentMode: local
        chunkSize:
          title: Document Chunk Size
          type: number
          description: Size of text chunks for embeddings
          default: 1000
        chunkOverlap:
          title: Chunk Overlap
          type: number
          description: Overlap between chunks
          default: 200

    - title: Repository Configuration
      properties:
        createGitRepo:
          title: Create New Git Repository
          type: boolean
          description: Publish to a new GitHub repository
          default: true
        repoUrl:
          title: Repository Location
          type: string
          ui:field: RepoUrlPicker
          ui:options:
            allowedHosts:
              - github.com
          ui:hidden:
            createGitRepo: false

  steps:
    - id: fetch-base
      name: Fetch RAG Quickstart
      action: fetch:plain
      input:
        url: https://github.com/rh-ai-quickstart/RAG
        targetPath: ./rag

    - id: create-ingestion-config
      name: Generate Ingestion Config
      action: fetch:template
      input:
        targetPath: ./rag/deploy/${{ parameters.deploymentMode }}
        values:
          dataSources: ${{ parameters.dataSources }}
          githubRepos: ${{ parameters.githubRepos }}
          githubToken: ${{ parameters.githubToken }}
          s3Endpoint: ${{ parameters.s3Endpoint }}
          s3Bucket: ${{ parameters.s3Bucket }}
          s3AccessKey: ${{ parameters.s3AccessKey }}
          s3SecretKey: ${{ parameters.s3SecretKey }}
          documentUrls: ${{ parameters.documentUrls }}
        content: |
          sources:
            {% if 'github' in parameters.dataSources %}
            {% for repo in parameters.githubRepos.split(',') %}
            - type: github
              repo: {{ repo.strip() }}
              {% if parameters.githubToken %}
              token: ${{ parameters.githubToken }}
              {% endif %}
            {% endfor %}
            {% endif %}

            {% if 's3-minio' in parameters.dataSources %}
            - type: s3
              endpoint: ${{ parameters.s3Endpoint }}
              bucket: ${{ parameters.s3Bucket }}
              access_key: ${{ parameters.s3AccessKey }}
              secret_key: ${{ parameters.s3SecretKey }}
            {% endif %}

            {% if 'url' in parameters.dataSources %}
            {% for url in parameters.documentUrls.split(',') %}
            - type: url
              url: {{ url.strip() }}
            {% endfor %}
            {% endif %}

          embedding:
            model: ${{ parameters.embeddingModel }}
            chunk_size: ${{ parameters.chunkSize }}
            chunk_overlap: ${{ parameters.chunkOverlap }}

    - id: create-helm-values
      name: Generate Helm Values
      if: ${{ parameters.deploymentMode == 'openshift' }}
      action: fetch:template
      input:
        targetPath: ./rag/deploy/openshift
        values:
          namespace: ${{ parameters.namespace }}
          llmModel: ${{ parameters.llmModel }}
          embeddingModel: ${{ parameters.embeddingModel }}
          accelerator: ${{ parameters.accelerator }}
          pgvectorStorageSize: ${{ parameters.pgvectorStorageSize }}
          pgvectorMemory: ${{ parameters.pgvectorMemory }}
          enableLlamaGuard: ${{ parameters.enableLlamaGuard }}
          enableStreamlit: ${{ parameters.enableStreamlit }}
        content: |
          # Helm values for RAG deployment
          namespace: ${{ parameters.namespace }}

          llm:
            model: ${{ parameters.llmModel }}
            accelerator: ${{ parameters.accelerator }}
            {% if parameters.enableLlamaGuard %}
            llamaGuard:
              enabled: true
            {% endif %}

          embedding:
            model: ${{ parameters.embeddingModel }}

          pgvector:
            storage:
              size: ${{ parameters.pgvectorStorageSize }}
            resources:
              requests:
                memory: ${{ parameters.pgvectorMemory }}

          ui:
            enabled: ${{ parameters.enableStreamlit }}
            port: ${{ parameters.uiPort }}

    - id: create-local-compose
      name: Generate Podman Compose Config
      if: ${{ parameters.deploymentMode == 'local' }}
      action: fetch:template
      input:
        targetPath: ./rag/deploy/local
        values:
          llmModel: ${{ parameters.llmModel }}
          embeddingModel: ${{ parameters.embeddingModel }}
          uiPort: ${{ parameters.uiPort }}
        content: |
          # Configuration for local Podman/Docker deployment
          # Run with: make start

          INFERENCE_MODEL=${{ parameters.llmModel }}
          EMBEDDING_MODEL=${{ parameters.embeddingModel }}
          UI_PORT=${{ parameters.uiPort }}

    - id: create-secrets
      name: Generate Secrets
      if: ${{ parameters.deploymentMode == 'openshift' }}
      action: fetch:template
      input:
        targetPath: ./rag/manifests
        values:
          namespace: ${{ parameters.namespace }}
          githubToken: ${{ parameters.githubToken }}
          s3AccessKey: ${{ parameters.s3AccessKey }}
          s3SecretKey: ${{ parameters.s3SecretKey }}
        content: |
          apiVersion: v1
          kind: Secret
          metadata:
            name: rag-credentials
            namespace: ${{ parameters.namespace }}
          type: Opaque
          stringData:
            {% if parameters.githubToken %}
            github-token: ${{ parameters.githubToken }}
            {% endif %}
            {% if parameters.s3AccessKey %}
            s3-access-key: ${{ parameters.s3AccessKey }}
            s3-secret-key: ${{ parameters.s3SecretKey }}
            {% endif %}

    - id: create-ingestion-cronjob
      name: Create Ingestion CronJob
      if: ${{ parameters.enableAutoIngestion and parameters.deploymentMode == 'openshift' }}
      action: fetch:template
      input:
        targetPath: ./rag/manifests
        values:
          namespace: ${{ parameters.namespace }}
          schedule: ${{ parameters.ingestionSchedule }}
        content: |
          apiVersion: batch/v1
          kind: CronJob
          metadata:
            name: rag-ingestion
            namespace: ${{ parameters.namespace }}
          spec:
            schedule: "${{ parameters.ingestionSchedule }}"
            jobTemplate:
              spec:
                template:
                  spec:
                    containers:
                      - name: ingestion
                        image: quay.io/rh-ai-quickstart/rag-ingestion:latest
                        envFrom:
                          - secretRef:
                              name: rag-credentials
                          - configMapRef:
                              name: ingestion-config
                    restartPolicy: OnFailure

    - id: create-readme
      name: Create Deployment README
      action: fetch:template
      input:
        targetPath: ./rag
        values:
          name: ${{ parameters.name }}
          deploymentMode: ${{ parameters.deploymentMode }}
          namespace: ${{ parameters.namespace }}
          llmModel: ${{ parameters.llmModel }}
        content: |
          # ${{ parameters.name }} - RAG Chatbot

          Deployment mode: **${{ parameters.deploymentMode }}**

          ## Quick Start

          {% if parameters.deploymentMode == 'local' %}
          ### Local Deployment

          ```bash
          cd deploy/local
          make start
          ```

          Access the UI at: http://localhost:${{ parameters.uiPort }}

          Useful commands:
          - `make status` - View service status
          - `make logs` - View logs
          - `make ingest` - Re-run ingestion
          - `make stop` - Stop all services
          - `make cleanup` - Reset everything
          {% else %}
          ### OpenShift Deployment

          ```bash
          # Login to OpenShift
          oc login --token=<token> --server=${{ parameters.openshiftCluster }}

          # Create project
          oc new-project ${{ parameters.namespace }}

          # Deploy using Helm
          helm install rag-chatbot deploy/openshift/helm \
            --namespace ${{ parameters.namespace }} \
            -f deploy/openshift/values.yaml

          # Wait for deployment
          oc wait --for=condition=available deployment/rag-chatbot -n ${{ parameters.namespace }}

          # Get route
          oc get route rag-ui -n ${{ parameters.namespace }}
          ```
          {% endif %}

          ## Configuration

          - **LLM Model**: ${{ parameters.llmModel }}
          - **Embedding Model**: ${{ parameters.embeddingModel }}
          - **Vector DB**: PGVector (PostgreSQL)
          - **Data Sources**: ${{ parameters.dataSources | join(', ') }}

          ## Architecture

          - **Frontend**: Streamlit UI
          - **LLM Serving**: {% if parameters.deploymentMode == 'local' %}Ollama{% else %}vLLM on OpenShift AI{% endif %}
          - **Vector Database**: PostgreSQL with PGVector
          - **Ingestion**: Docling for PDF processing
          - **Orchestration**: {% if parameters.enableKubeflowPipelines %}Kubeflow Pipelines{% else %}Direct ingestion{% endif %}

          ## Documentation

          - [RAG Quickstart Docs](https://github.com/rh-ai-quickstart/RAG)
          - [Local Setup Guide](https://github.com/rh-ai-quickstart/RAG/blob/main/docs/local_setup_guide.md)
          - [OpenShift AI Docs](https://docs.redhat.com/en/documentation/red_hat_openshift_ai)

    - id: create-catalog-info
      name: Create Catalog Info
      action: fetch:template
      input:
        targetPath: ./rag
        values:
          name: ${{ parameters.name }}
          description: ${{ parameters.description }}
          owner: ${{ parameters.owner }}
          namespace: ${{ parameters.namespace }}
          openshiftCluster: ${{ parameters.openshiftCluster }}
          deploymentMode: ${{ parameters.deploymentMode }}
        content: |
          apiVersion: backstage.io/v1alpha1
          kind: Component
          metadata:
            name: ${{ parameters.name }}
            description: ${{ parameters.description }}
            annotations:
              {% if parameters.deploymentMode == 'openshift' %}
              backstage.io/kubernetes-namespace: ${{ parameters.namespace }}
              openshift.io/cluster: ${{ parameters.openshiftCluster }}
              {% endif %}
              github.com/project-slug: ${{ parameters.repoUrl | parseRepoUrl }}
              docs.redhat.com/quickstart-type: enterprise-rag
              docs.redhat.com/deployment-mode: ${{ parameters.deploymentMode }}
            tags:
              - ai
              - rag
              - pgvector
              - llama-stack
              - ${{ parameters.deploymentMode }}
            links:
              - url: https://github.com/rh-ai-quickstart/RAG
                title: RAG Quickstart Repository
                icon: github
              - url: https://github.com/rh-ai-quickstart/RAG/blob/main/docs/local_setup_guide.md
                title: Setup Guide
                icon: docs
          spec:
            type: service
            lifecycle: experimental
            owner: ${{ parameters.owner }}
            system: ai-platform
            dependsOn:
              - resource:pgvector
              {% if parameters.deploymentMode == 'openshift' %}
              - resource:openshift-ai
              {% else %}
              - resource:ollama
              {% endif %}

    - id: publish
      name: Publish to GitHub
      if: ${{ parameters.createGitRepo }}
      action: publish:github
      input:
        allowedHosts: ['github.com']
        description: ${{ parameters.description }}
        repoUrl: ${{ parameters.repoUrl }}
        defaultBranch: main
        repoVisibility: private
        sourcePath: ./rag

    - id: register
      name: Register Component
      if: ${{ parameters.createGitRepo }}
      action: catalog:register
      input:
        repoContentsUrl: ${{ steps.publish.output.repoContentsUrl }}
        catalogInfoPath: '/catalog-info.yaml'

  output:
    links:
      - title: Repository
        url: ${{ steps.publish.output.remoteUrl }}
        icon: github
      - title: Open in Catalog
        icon: catalog
        entityRef: ${{ steps.register.output.entityRef }}
      - title: OpenShift Console
        url: ${{ parameters.openshiftCluster }}/k8s/cluster/projects/${{ parameters.namespace }}
        icon: dashboard
        condition: ${{ parameters.deploymentMode == 'openshift' }}
      - title: RAG Quickstart Docs
        url: https://github.com/rh-ai-quickstart/RAG
        icon: docs
      - title: Local Setup Guide
        url: https://github.com/rh-ai-quickstart/RAG/blob/main/docs/local_setup_guide.md
        icon: docs
    text:
      - title: Next Steps
        content: |
          Your Enterprise RAG Chatbot has been configured!

          Deployment Mode: ${{ parameters.deploymentMode }}
          LLM: ${{ parameters.llmModel }}
          Embedding: ${{ parameters.embeddingModel }}
          Vector DB: PGVector (PostgreSQL)

          {% if parameters.deploymentMode == 'local' %}
          ## Local Deployment

          ```bash
          cd deploy/local
          make start
          ```

          Access the UI at: http://localhost:${{ parameters.uiPort }}

          Available services:
          - RAG UI: http://localhost:8501
          - Llama Stack API: http://localhost:8321
          - Ollama API: http://localhost:11434
          - PGVector: localhost:5432

          {% else %}
          ## OpenShift Deployment

          1. Login to OpenShift: `oc login --token=<token> --server=${{ parameters.openshiftCluster }}`
          2. Deploy: `helm install rag-chatbot deploy/openshift/helm -n ${{ parameters.namespace }}`
          3. Monitor: `oc get pods -n ${{ parameters.namespace }} -w`
          4. Access UI via OpenShift route

          Hardware: ${{ parameters.accelerator }}
          {% endif %}

          ## Documentation

          - [RAG Quickstart](https://github.com/rh-ai-quickstart/RAG)
          - [Setup Guide](https://github.com/rh-ai-quickstart/RAG/blob/main/docs/local_setup_guide.md)
