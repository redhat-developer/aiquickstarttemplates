apiVersion: scaffolder.backstage.io/v1beta3
kind: Template
metadata:
  name: redhat-ai-llm-cpu-serving
  title: LLM CPU Serving - HR Assistant
  description: Deploy an AI-powered HR Assistant using vLLM on CPU (no GPU required) based on rh-ai-quickstart/llm-cpu-serving
  tags:
    - redhat
    - ai
    - openshift-ai
    - llm
    - cpu-serving
    - vllm
    - anythingllm
    - hr-assistant
spec:
  owner: platform-team
  type: service

  parameters:
    - title: Project Information
      required:
        - name
        - owner
      properties:
        name:
          title: Project Name
          type: string
          description: Name for your HR Assistant instance
          ui:autofocus: true
          ui:field: EntityNamePicker
          default: hr-assistant
        owner:
          title: Owner
          type: string
          description: Team or individual owning this service
          ui:field: OwnerPicker
          ui:options:
            catalogFilter:
              kind: [Group, User]
        description:
          title: Description
          type: string
          description: Purpose of this HR Assistant deployment
          default: AI-powered HR Assistant on CPU using TinyLlama

    - title: OpenShift Configuration
      required:
        - openshiftCluster
        - namespace
      properties:
        openshiftCluster:
          title: OpenShift Cluster
          type: string
          description: Target OpenShift cluster URL
          default: https://api.cluster.example.com:6443
          ui:help: Requires OpenShift 4.16.24+
        namespace:
          title: Namespace/Project
          type: string
          description: OpenShift project for deployment
          default: hr-assistant
        openshiftAIVersion:
          title: OpenShift AI Version
          type: string
          description: OpenShift AI version
          default: 2.16.2+
          enum:
            - 2.16.2+
            - 2.17+
          ui:help: Requires OpenShift AI 2.16.2 or later

    - title: Model Configuration
      properties:
        llmModel:
          title: LLM Model
          type: string
          description: Language model for serving
          default: TinyLlama-1.1B-Chat-v1.0
          enum:
            - TinyLlama-1.1B-Chat-v1.0
          enumNames:
            - TinyLlama 1.1B (Default - CPU optimized)
          ui:help: Currently only TinyLlama is supported for CPU serving
        vllmImage:
          title: vLLM Container Image
          type: string
          description: vLLM CPU runtime container image
          default: quay.io/rh-aiservices-bu/vllm-cpu-openai-ubi9:latest
          ui:help: Pre-built vLLM CPU image from Red Hat AI Services

    - title: Resource Configuration
      properties:
        cpuRequest:
          title: CPU Request
          type: string
          description: Requested CPU cores
          default: "2"
          enum:
            - "2"
            - "4"
            - "8"
          enumNames:
            - 2 cores (Minimum)
            - 4 cores (Recommended)
            - 8 cores (Optimal)
        memoryRequest:
          title: Memory Request
          type: string
          description: Requested memory
          default: 4Gi
          enum:
            - 4Gi
            - 8Gi
          enumNames:
            - 4 GB (Minimum)
            - 8 GB (Recommended)
        cpuArchitecture:
          title: CPU Architecture
          type: string
          description: Preferred CPU architecture
          default: x86_64
          enum:
            - x86_64
            - x86_64-avx512
          enumNames:
            - Standard x86_64
            - Intel with AVX-512 (Recommended for compressed models)

    - title: AnythingLLM Configuration
      properties:
        enableAnythingLLM:
          title: Deploy AnythingLLM Workbench
          type: boolean
          description: Deploy AnythingLLM chat interface
          default: true
          ui:help: Provides the chat UI for HR assistant
        anythingLLMWorkspace:
          title: Workspace Name
          type: string
          description: AnythingLLM workspace name
          default: Assistant to the HR Representative
          ui:hidden:
            enableAnythingLLM: false

    - title: Prerequisites Check
      properties:
        hasServiceMesh:
          title: OpenShift Service Mesh Installed
          type: boolean
          description: Confirm OpenShift Service Mesh is installed
          default: false
          ui:help: Required for deployment
        hasServerless:
          title: OpenShift Serverless Installed
          type: boolean
          description: Confirm OpenShift Serverless is installed
          default: false
          ui:help: Required for deployment

    - title: Repository Configuration
      properties:
        createGitRepo:
          title: Create New Git Repository
          type: boolean
          description: Publish to a new GitHub repository
          default: true
        repoUrl:
          title: Repository Location
          type: string
          ui:field: RepoUrlPicker
          ui:options:
            allowedHosts:
              - github.com
          ui:hidden:
            createGitRepo: false

  steps:
    - id: fetch-base
      name: Fetch LLM CPU Serving Quickstart
      action: fetch:plain
      input:
        url: https://github.com/rh-ai-quickstart/llm-cpu-serving
        targetPath: ./llm-cpu-serving

    - id: create-helm-values
      name: Generate Helm Values
      action: fetch:template
      input:
        targetPath: ./llm-cpu-serving
        values:
          namespace: ${{ parameters.namespace }}
          projectName: ${{ parameters.name }}
          llmModel: ${{ parameters.llmModel }}
          vllmImage: ${{ parameters.vllmImage }}
          cpuRequest: ${{ parameters.cpuRequest }}
          memoryRequest: ${{ parameters.memoryRequest }}
          enableAnythingLLM: ${{ parameters.enableAnythingLLM }}
          workspaceName: ${{ parameters.anythingLLMWorkspace }}
        content: |
          # Custom values for ${{ parameters.name }}
          # Based on rh-ai-quickstart/llm-cpu-serving

          namespace: ${{ parameters.namespace }}
          projectName: ${{ parameters.name }}

          vllm:
            image: ${{ parameters.vllmImage }}
            model: ${{ parameters.llmModel }}
            resources:
              requests:
                cpu: ${{ parameters.cpuRequest }}
                memory: ${{ parameters.memoryRequest }}
              limits:
                cpu: "{{ parameters.cpuRequest * 2 }}"
                memory: "{{ parameters.memoryRequest * 2 }}"

          anythingllm:
            enabled: ${{ parameters.enableAnythingLLM }}
            workspace: "${{ parameters.anythingLLMWorkspace }}"

    - id: create-deployment-script
      name: Create Deployment Script
      action: fetch:template
      input:
        targetPath: ./llm-cpu-serving
        values:
          namespace: ${{ parameters.namespace }}
          projectName: ${{ parameters.name }}
          openshiftCluster: ${{ parameters.openshiftCluster }}
        content: |
          #!/bin/bash
          # Deployment script for ${{ parameters.name }}

          set -e

          PROJECT="${{ parameters.name }}"
          NAMESPACE="${{ parameters.namespace }}"

          echo "ğŸš€ Deploying HR Assistant: ${PROJECT}"

          # Create OpenShift project
          echo "ğŸ“¦ Creating project ${NAMESPACE}..."
          oc new-project ${NAMESPACE} || echo "Project already exists"

          # Install via Helm
          echo "âš™ï¸  Installing Helm chart..."
          helm install ${PROJECT} helm/ --namespace ${NAMESPACE} -f values.yaml

          # Monitor deployment
          echo "ğŸ‘€ Monitoring pod status..."
          oc -n ${NAMESPACE} get pods -w

          echo "âœ… Deployment complete!"
          echo ""
          echo "Expected pods:"
          echo "  - anythingllm"
          echo "  - anythingllm-seed"
          echo "  - tinyllama-1b-cpu-predictor"
          echo ""
          echo "Access your HR Assistant via OpenShift AI Dashboard"

    - id: create-readme
      name: Create Deployment README
      action: fetch:template
      input:
        targetPath: ./llm-cpu-serving
        values:
          name: ${{ parameters.name }}
          namespace: ${{ parameters.namespace }}
          openshiftCluster: ${{ parameters.openshiftCluster }}
          cpuRequest: ${{ parameters.cpuRequest }}
          memoryRequest: ${{ parameters.memoryRequest }}
        content: |
          # ${{ parameters.name }} - HR Assistant (LLM CPU Serving)

          AI-powered HR Assistant running on CPU infrastructure (no GPU required).

          ## Overview

          - **Model**: TinyLlama 1.1B
          - **Runtime**: vLLM CPU
          - **UI**: AnythingLLM Workbench
          - **Resources**: ${{ parameters.cpuRequest }} CPU, ${{ parameters.memoryRequest }} Memory

          ## Quick Start

          ### Prerequisites

          Ensure you have the following installed on OpenShift:
          - âœ… OpenShift 4.16.24+
          - âœ… OpenShift AI 2.16.2+
          - âœ… OpenShift Service Mesh
          - âœ… OpenShift Serverless

          ### Deployment

          ```bash
          # Login to OpenShift
          oc login --token=<token> --server=${{ parameters.openshiftCluster }}

          # Run deployment script
          chmod +x deploy.sh
          ./deploy.sh
          ```

          **OR manually:**

          ```bash
          # Create project
          oc new-project ${{ parameters.namespace }}

          # Install via Helm
          helm install ${{ parameters.name }} helm/ \
            --namespace ${{ parameters.namespace }} \
            -f values.yaml

          # Monitor pods
          oc -n ${{ parameters.namespace }} get pods -w
          ```

          ### Expected Pods

          Wait for these pods to be Running:
          - `anythingllm` - Chat interface workbench
          - `anythingllm-seed` - Initial workspace setup
          - `tinyllama-1b-cpu-predictor` - vLLM inference server

          ### Access the HR Assistant

          1. Navigate to OpenShift AI Dashboard
          2. Go to your project: `${{ parameters.namespace }}`
          3. Open the AnythingLLM workbench
          4. Start chatting with the HR Assistant!

          ## Use Case

          The HR Assistant helps HR representatives by:
          - Answering policy questions
          - Providing guidance on procedures
          - Citing relevant documentation
          - Supporting decision-making

          ## Architecture

          ```
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   AnythingLLM Workbench (UI)        â”‚
          â”‚   Chat interface for HR queries     â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   vLLM CPU Runtime                  â”‚
          â”‚   TinyLlama 1.1B Inference          â”‚
          â”‚   OpenAI-compatible API             â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          ```

          ## Cleanup

          ```bash
          helm uninstall ${{ parameters.name }} --namespace ${{ parameters.namespace }}
          oc delete project ${{ parameters.namespace }}
          ```

          ## Technical Details

          - **Container Image**: quay.io/rh-aiservices-bu/vllm-cpu-openai-ubi9
          - **Model Size**: 1.1B parameters
          - **Storage**: ~5 GB
          - **CPU Optimization**: Built with vLLM CPU support
          - **Recommended CPU**: Intel with AVX-512 for best performance

          ## References

          - [rh-ai-quickstart/llm-cpu-serving](https://github.com/rh-ai-quickstart/llm-cpu-serving)
          - [vLLM CPU Documentation](https://docs.vllm.ai/en/latest/getting_started/cpu-installation.html)
          - [llm-on-openshift](https://github.com/rh-aiservices-bu/llm-on-openshift)

    - id: create-catalog-info
      name: Create Catalog Info
      action: fetch:template
      input:
        targetPath: ./llm-cpu-serving
        values:
          name: ${{ parameters.name }}
          description: ${{ parameters.description }}
          owner: ${{ parameters.owner }}
          namespace: ${{ parameters.namespace }}
          openshiftCluster: ${{ parameters.openshiftCluster }}
        content: |
          apiVersion: backstage.io/v1alpha1
          kind: Component
          metadata:
            name: ${{ parameters.name }}
            description: ${{ parameters.description }}
            annotations:
              backstage.io/kubernetes-namespace: ${{ parameters.namespace }}
              openshift.io/cluster: ${{ parameters.openshiftCluster }}
              github.com/project-slug: ${{ parameters.repoUrl | parseRepoUrl }}
              docs.redhat.com/quickstart-type: llm-cpu-serving
            tags:
              - ai
              - llm
              - cpu-serving
              - vllm
              - anythingllm
              - hr-assistant
              - openshift-ai
            links:
              - url: https://github.com/rh-ai-quickstart/llm-cpu-serving
                title: LLM CPU Serving Repository
                icon: github
              - url: https://github.com/rh-aiservices-bu/llm-on-openshift
                title: LLM on OpenShift
                icon: github
          spec:
            type: service
            lifecycle: production
            owner: ${{ parameters.owner }}
            system: ai-platform
            dependsOn:
              - resource:openshift-ai
              - resource:service-mesh
              - resource:serverless

    - id: publish
      name: Publish to GitHub
      if: ${{ parameters.createGitRepo }}
      action: publish:github
      input:
        allowedHosts: ['github.com']
        description: ${{ parameters.description }}
        repoUrl: ${{ parameters.repoUrl }}
        defaultBranch: main
        repoVisibility: private
        sourcePath: ./llm-cpu-serving

    - id: register
      name: Register Component
      if: ${{ parameters.createGitRepo }}
      action: catalog:register
      input:
        repoContentsUrl: ${{ steps.publish.output.repoContentsUrl }}
        catalogInfoPath: '/catalog-info.yaml'

  output:
    links:
      - title: Repository
        url: ${{ steps.publish.output.remoteUrl }}
        icon: github
      - title: Open in Catalog
        icon: catalog
        entityRef: ${{ steps.register.output.entityRef }}
      - title: OpenShift Console
        url: ${{ parameters.openshiftCluster }}/k8s/cluster/projects/${{ parameters.namespace }}
        icon: dashboard
      - title: LLM CPU Serving Docs
        url: https://github.com/rh-ai-quickstart/llm-cpu-serving
        icon: docs
    text:
      - title: Next Steps
        content: |
          Your HR Assistant has been configured!

          **Model**: TinyLlama 1.1B
          **Runtime**: vLLM CPU
          **Resources**: ${{ parameters.cpuRequest }} CPU, ${{ parameters.memoryRequest }} Memory

          ## Prerequisites Check

          Before deploying, ensure:
          - [ ] OpenShift 4.16.24+ is running
          - [ ] OpenShift AI 2.16.2+ is installed
          - [ ] Service Mesh: ${{ parameters.hasServiceMesh ? 'âœ… Confirmed' : 'âŒ Not confirmed' }}
          - [ ] Serverless: ${{ parameters.hasServerless ? 'âœ… Confirmed' : 'âŒ Not confirmed' }}

          ## Deploy Now

          ```bash
          cd ${{ parameters.name }}
          chmod +x deploy.sh
          ./deploy.sh
          ```

          ## Monitor Progress

          ```bash
          oc -n ${{ parameters.namespace }} get pods -w
          ```

          Expected pods:
          - anythingllm
          - anythingllm-seed
          - tinyllama-1b-cpu-predictor

          ## Access Your HR Assistant

          Navigate to OpenShift AI Dashboard â†’ ${{ parameters.namespace }} â†’ AnythingLLM Workbench

          **Estimated deployment time**: 5-10 minutes
