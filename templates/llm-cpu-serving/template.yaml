apiVersion: scaffolder.backstage.io/v1beta3
kind: Template
metadata:
  name: redhat-ai-llm-cpu-serving
  title: LLM CPU Serving
  description: Deploy a lightweight AI assistant serving small language models on CPU
  tags:
    - redhat
    - ai
    - llm
    - cpu-serving
    - openshift
    - lightweight
spec:
  owner: platform-team
  type: service

  parameters:
    - title: Project Information
      required:
        - name
        - owner
      properties:
        name:
          title: Project Name
          type: string
          description: Name for your LLM CPU serving instance
          ui:autofocus: true
          ui:field: EntityNamePicker
        owner:
          title: Owner
          type: string
          description: Team or individual owning this service
          ui:field: OwnerPicker
          ui:options:
            catalogFilter:
              kind: [Group, User]
        description:
          title: Description
          type: string
          description: Purpose of this LLM service
          default: Lightweight CPU-based LLM serving for AI assistance

    - title: Deployment Configuration
      required:
        - openshiftCluster
        - namespace
      properties:
        openshiftCluster:
          title: OpenShift Cluster
          type: string
          description: Target OpenShift cluster URL
          default: https://api.cluster.example.com:6443
        namespace:
          title: Namespace
          type: string
          description: OpenShift namespace for deployment
          default: llm-serving

    - title: Model Configuration
      required:
        - modelSize
      properties:
        modelSize:
          title: Model Size
          type: string
          description: Choose model size based on CPU resources
          default: small-1b
          enum:
            - small-1b
            - medium-3b
            - large-7b
          enumNames:
            - Small (1B - 2 CPU cores, 4GB RAM)
            - Medium (3B - 4 CPU cores, 8GB RAM)
            - Large (7B - 8 CPU cores, 16GB RAM)
        modelSource:
          title: Model Source
          type: string
          description: Source for the language model
          default: huggingface
          enum:
            - huggingface
            - custom
          enumNames:
            - HuggingFace Hub
            - Custom Model Path
        huggingfaceModel:
          title: HuggingFace Model
          type: string
          description: HuggingFace model identifier
          default: TinyLlama/TinyLlama-1.1B-Chat-v1.0
          ui:hidden:
            modelSource: custom
        customModelPath:
          title: Custom Model Path
          type: string
          description: S3 or local path to custom model
          ui:hidden:
            modelSource: huggingface
        quantization:
          title: Model Quantization
          type: string
          description: Quantization method for better CPU performance
          default: int8
          enum:
            - none
            - int8
            - int4
          enumNames:
            - None (Full precision)
            - INT8 (2x faster, minimal quality loss)
            - INT4 (4x faster, some quality loss)

    - title: Serving Configuration
      properties:
        maxConcurrentRequests:
          title: Max Concurrent Requests
          type: number
          description: Maximum number of concurrent requests
          default: 4
        maxTokens:
          title: Max Tokens per Request
          type: number
          description: Maximum tokens in model response
          default: 512
        temperature:
          title: Default Temperature
          type: number
          description: Sampling temperature (0.0 - 2.0)
          default: 0.7
          minimum: 0
          maximum: 2
        enableCaching:
          title: Enable Response Caching
          type: boolean
          description: Cache responses for identical requests
          default: true

    - title: Resource Limits
      properties:
        cpuRequest:
          title: CPU Request
          type: string
          description: Requested CPU cores
          default: "2"
        cpuLimit:
          title: CPU Limit
          type: string
          description: Maximum CPU cores
          default: "4"
        memoryRequest:
          title: Memory Request
          type: string
          description: Requested memory
          default: 4Gi
        memoryLimit:
          title: Memory Limit
          type: string
          description: Maximum memory
          default: 8Gi

    - title: API Configuration
      properties:
        enableAuth:
          title: Enable API Authentication
          type: boolean
          description: Require API key for requests
          default: true
        apiKeys:
          title: API Keys
          type: string
          description: Comma-separated list of API keys
          ui:widget: password
          ui:hidden:
            enableAuth: false
        enableRateLimit:
          title: Enable Rate Limiting
          type: boolean
          description: Limit requests per client
          default: true
        rateLimit:
          title: Rate Limit
          type: string
          description: Requests per minute per client
          default: "60"
          ui:hidden:
            enableRateLimit: false

    - title: Repository Configuration
      properties:
        createGitRepo:
          title: Create New Git Repository
          type: boolean
          description: Publish to a new GitHub repository
          default: true
        repoUrl:
          title: Repository Location
          type: string
          ui:field: RepoUrlPicker
          ui:options:
            allowedHosts:
              - github.com
          ui:hidden:
            createGitRepo: false

  steps:
    - id: fetch-base
      name: Fetch LLM CPU Serving
      action: fetch:plain
      input:
        url: https://github.com/rh-ai-quickstart/llm-cpu-serving
        targetPath: ./llm-serving

    - id: create-deployment
      name: Generate Deployment Manifest
      action: fetch:template
      input:
        targetPath: ./llm-serving/manifests
        values:
          name: ${{ parameters.name }}
          namespace: ${{ parameters.namespace }}
          modelSize: ${{ parameters.modelSize }}
          huggingfaceModel: ${{ parameters.huggingfaceModel }}
          quantization: ${{ parameters.quantization }}
          cpuRequest: ${{ parameters.cpuRequest }}
          cpuLimit: ${{ parameters.cpuLimit }}
          memoryRequest: ${{ parameters.memoryRequest }}
          memoryLimit: ${{ parameters.memoryLimit }}
          maxConcurrentRequests: ${{ parameters.maxConcurrentRequests }}
          maxTokens: ${{ parameters.maxTokens }}
          temperature: ${{ parameters.temperature }}
        content: |
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: ${{ parameters.name }}
            namespace: ${{ parameters.namespace }}
          spec:
            replicas: 1
            selector:
              matchLabels:
                app: ${{ parameters.name }}
            template:
              metadata:
                labels:
                  app: ${{ parameters.name }}
              spec:
                containers:
                  - name: llm-server
                    image: quay.io/redhat-ai/llm-cpu-serving:latest
                    env:
                      - name: MODEL_ID
                        value: ${{ parameters.huggingfaceModel }}
                      - name: QUANTIZATION
                        value: ${{ parameters.quantization }}
                      - name: MAX_CONCURRENT_REQUESTS
                        value: "${{ parameters.maxConcurrentRequests }}"
                      - name: MAX_TOKENS
                        value: "${{ parameters.maxTokens }}"
                      - name: TEMPERATURE
                        value: "${{ parameters.temperature }}"
                      {% if parameters.enableCaching %}
                      - name: ENABLE_CACHE
                        value: "true"
                      {% endif %}
                    resources:
                      requests:
                        cpu: ${{ parameters.cpuRequest }}
                        memory: ${{ parameters.memoryRequest }}
                      limits:
                        cpu: ${{ parameters.cpuLimit }}
                        memory: ${{ parameters.memoryLimit }}
                    ports:
                      - containerPort: 8080
                        name: http

    - id: create-service
      name: Generate Service Manifest
      action: fetch:template
      input:
        targetPath: ./llm-serving/manifests
        values:
          name: ${{ parameters.name }}
          namespace: ${{ parameters.namespace }}
        content: |
          apiVersion: v1
          kind: Service
          metadata:
            name: ${{ parameters.name }}
            namespace: ${{ parameters.namespace }}
          spec:
            selector:
              app: ${{ parameters.name }}
            ports:
              - protocol: TCP
                port: 8080
                targetPort: 8080
            type: ClusterIP

    - id: create-route
      name: Generate Route Manifest
      action: fetch:template
      input:
        targetPath: ./llm-serving/manifests
        values:
          name: ${{ parameters.name }}
          namespace: ${{ parameters.namespace }}
        content: |
          apiVersion: route.openshift.io/v1
          kind: Route
          metadata:
            name: ${{ parameters.name }}
            namespace: ${{ parameters.namespace }}
          spec:
            to:
              kind: Service
              name: ${{ parameters.name }}
            port:
              targetPort: 8080
            tls:
              termination: edge
              insecureEdgeTerminationPolicy: Redirect

    - id: create-secrets
      name: Generate Secrets
      if: ${{ parameters.enableAuth }}
      action: fetch:template
      input:
        targetPath: ./llm-serving/manifests
        values:
          namespace: ${{ parameters.namespace }}
          apiKeys: ${{ parameters.apiKeys }}
        content: |
          apiVersion: v1
          kind: Secret
          metadata:
            name: api-credentials
            namespace: ${{ parameters.namespace }}
          type: Opaque
          stringData:
            api-keys: ${{ parameters.apiKeys }}

    - id: create-catalog-info
      name: Create Catalog Info
      action: fetch:template
      input:
        targetPath: ./llm-serving
        values:
          name: ${{ parameters.name }}
          description: ${{ parameters.description }}
          owner: ${{ parameters.owner }}
          namespace: ${{ parameters.namespace }}
          openshiftCluster: ${{ parameters.openshiftCluster }}
        content: |
          apiVersion: backstage.io/v1alpha1
          kind: Component
          metadata:
            name: ${{ parameters.name }}
            description: ${{ parameters.description }}
            annotations:
              backstage.io/kubernetes-namespace: ${{ parameters.namespace }}
              openshift.io/cluster: ${{ parameters.openshiftCluster }}
              github.com/project-slug: ${{ parameters.repoUrl | parseRepoUrl }}
              docs.redhat.com/quickstart-type: llm-cpu-serving
            tags:
              - ai
              - llm
              - cpu-serving
              - lightweight
            links:
              - url: https://docs.redhat.com/en/learn/ai-quickstarts
                title: LLM CPU Serving Documentation
                icon: docs
          spec:
            type: service
            lifecycle: production
            owner: ${{ parameters.owner }}
            system: ai-platform
            providesApis:
              - llm-inference-api

    - id: publish
      name: Publish to GitHub
      if: ${{ parameters.createGitRepo }}
      action: publish:github
      input:
        allowedHosts: ['github.com']
        description: ${{ parameters.description }}
        repoUrl: ${{ parameters.repoUrl }}
        defaultBranch: main
        repoVisibility: private
        sourcePath: ./llm-serving

    - id: register
      name: Register Component
      if: ${{ parameters.createGitRepo }}
      action: catalog:register
      input:
        repoContentsUrl: ${{ steps.publish.output.repoContentsUrl }}
        catalogInfoPath: '/catalog-info.yaml'

  output:
    links:
      - title: Repository
        url: ${{ steps.publish.output.remoteUrl }}
        icon: github
      - title: Open in Catalog
        icon: catalog
        entityRef: ${{ steps.register.output.entityRef }}
      - title: OpenShift Console
        url: ${{ parameters.openshiftCluster }}/k8s/cluster/projects/${{ parameters.namespace }}
        icon: dashboard
    text:
      - title: Next Steps
        content: |
          Your LLM CPU Serving instance has been configured!

          Model: ${{ parameters.huggingfaceModel }}
          Size: ${{ parameters.modelSize }}
          Quantization: ${{ parameters.quantization }}
          Resources: ${{ parameters.cpuRequest }} CPU, ${{ parameters.memoryRequest }} Memory

          Next steps:
          1. Deploy to OpenShift: kubectl apply -k manifests/
          2. Wait for pod to be ready (model download may take a few minutes)
          3. Get the route URL: oc get route ${{ parameters.name }} -n ${{ parameters.namespace }}
          4. Test the API endpoint
          5. Integrate with your applications

          API Endpoint: https://<route-url>/v1/completions
