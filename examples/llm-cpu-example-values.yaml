# Example parameter values for LLM CPU Serving - HR Assistant template
# Based on rh-ai-quickstart/llm-cpu-serving repository
# Use these as a reference when filling out the template form

name: "hr-assistant"
owner: "hr-team"
description: "AI-powered HR Assistant on CPU using TinyLlama"

# OpenShift Configuration
openshiftCluster: "https://api.prod-cluster.example.com:6443"
namespace: "hr-assistant"
openshiftAIVersion: "2.16.2+"

# Model Configuration
llmModel: "TinyLlama-1.1B-Chat-v1.0"  # Only supported model for CPU serving
vllmImage: "quay.io/rh-aiservices-bu/vllm-cpu-openshift-ubi9:latest"

# Resource Configuration
cpuRequest: "4"  # Options: "2" (min), "4" (recommended), "8" (optimal)
memoryRequest: "8Gi"  # Options: "4Gi" (min), "8Gi" (recommended)
cpuArchitecture: "x86_64-avx512"  # Use AVX-512 for best performance

# AnythingLLM Configuration
enableAnythingLLM: true
anythingLLMWorkspace: "Assistant to the HR Representative"

# Prerequisites Check (REQUIRED!)
hasServiceMesh: true  # OpenShift Service Mesh must be installed
hasServerless: true   # OpenShift Serverless must be installed

# Repository
createGitRepo: true
repoUrl: "github.com/your-org/hr-assistant"

# ---
# Deployment will create these pods:
# - anythingllm (Chat UI workbench)
# - anythingllm-seed (Workspace setup)
# - tinyllama-1b-cpu-predictor (vLLM inference server)
#
# Access via OpenShift AI Dashboard → hr-assistant project → AnythingLLM workbench
#
# Estimated deployment time: 5-10 minutes
